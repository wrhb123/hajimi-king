import json
import os
from datetime import datetime
from typing import Dict, List, Optional, Any, Set
from dataclasses import dataclass, field

from common.Logger import logger
from common.config import Config


@dataclass
class Checkpoint:
    last_scan_time: Optional[str] = None
    scanned_shas: Set[str] = field(default_factory=set)
    processed_queries: Set[str] = field(default_factory=set)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼Œä½†ä¸åŒ…å«scanned_shasï¼ˆå•ç‹¬å­˜å‚¨ï¼‰"""
        return {
            "last_scan_time": self.last_scan_time,
            "processed_queries": list(self.processed_queries)
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Checkpoint':
        """ä»å­—å…¸åˆ›å»ºCheckpointå¯¹è±¡ï¼Œscanned_shaséœ€è¦å•ç‹¬åŠ è½½"""
        return cls(
            last_scan_time=data.get("last_scan_time"),
            scanned_shas=set(),  # å°†é€šè¿‡FileManagerå•ç‹¬åŠ è½½
            processed_queries=set(data.get("processed_queries", []))
        )
    
    def add_scanned_sha(self, sha: str) -> None:
        if sha:
            self.scanned_shas.add(sha)
    
    def add_processed_query(self, query: str) -> None:
        if query:
            self.processed_queries.add(query)
    
    def update_scan_time(self) -> None:
        self.last_scan_time = datetime.utcnow().isoformat()


class FileManager:
    """æ–‡ä»¶ç®¡ç†å™¨ï¼šè´Ÿè´£æ‰€æœ‰æ–‡ä»¶ç›¸å…³æ“ä½œ"""
    
    def __init__(self, data_dir: str):
        """
        åˆå§‹åŒ–FileManagerå¹¶å®Œæˆæ‰€æœ‰å¿…è¦çš„è®¾ç½®
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        logger.info("ğŸ”§ Initializing FileManager...")
        
        # 1. åŸºç¡€è·¯å¾„è®¾ç½®
        self.data_dir = data_dir
        self.checkpoint_file = os.path.join(data_dir, "checkpoint.json")
        self.scanned_shas_file = os.path.join(data_dir, Config.SCANNED_SHAS_FILE)
        
        # 2. åŠ¨æ€æ–‡ä»¶å
        self._detail_log_filename: Optional[str] = None
        self._keys_valid_filename: Optional[str] = None
        self._rate_limited_filename: Optional[str] = None
        self._rate_limited_detail_filename: Optional[str] = None
        
        # 3. åˆ›å»ºæ•°æ®ç›®å½•
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir, exist_ok=True)
            logger.info(f"Created data directory: {self.data_dir}")
        else:
            logger.info(f"Data directory exists: {self.data_dir}")
        
        # 4. åŠ è½½æœç´¢æŸ¥è¯¢
        try:
            self._search_queries = self.load_search_queries(Config.QUERIES_FILE)
            logger.info(f"âœ… Loaded {len(self._search_queries)} search queries")
        except Exception as e:
            logger.error(f"âŒ Failed to load search queries: {e}")
            self._search_queries = []
        
        # 5. åˆå§‹åŒ–æ–‡ä»¶å
        start_time = datetime.now()
        detail_prefix = Config.VALID_KEY_DETAIL_PREFIX.rstrip('_')
        self._detail_log_filename = os.path.join(
            self.data_dir, 
            f"{detail_prefix}_{start_time.strftime('%Y%m%d')}.log"
        )
        
        self._keys_valid_filename = os.path.join(
            self.data_dir, 
            f"{Config.VALID_KEY_PREFIX}{start_time.strftime('%Y%m%d_%H')}.txt"
        )
        
        self._rate_limited_filename = os.path.join(
            self.data_dir, 
            f"{Config.RATE_LIMITED_KEY_PREFIX}{start_time.strftime('%Y%m%d_%H')}.txt"
        )
        
        self._rate_limited_detail_filename = os.path.join(
            self.data_dir, 
            f"{Config.RATE_LIMITED_KEY_DETAIL_PREFIX}{start_time.strftime('%Y%m%d')}.log"
        )
        
        # åˆ›å»ºæ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œå…ˆç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        for filename in [self._detail_log_filename, self._keys_valid_filename, self._rate_limited_filename, self._rate_limited_detail_filename]:
            if not os.path.exists(filename):
                # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨ï¼ˆç±»ä¼¼ mkdir -pï¼‰
                parent_dir = os.path.dirname(filename)
                if parent_dir:
                    os.makedirs(parent_dir, exist_ok=True)
                
                with open(filename, 'a', encoding='utf-8') as f:
                    f.write("")
        
        logger.info(f"Initialized detail log filename: {self._detail_log_filename}")
        logger.info(f"Initialized keys valid filename: {self._keys_valid_filename}")
        logger.info(f"Initialized rate limited filename: {self._rate_limited_filename}")
        logger.info(f"Initialized rate limited detail filename: {self._rate_limited_detail_filename}")
        
        logger.info("âœ… FileManager initialization complete")
    
    def check(self) -> bool:
        """
        æ£€æŸ¥FileManageræ˜¯å¦æ­£ç¡®åˆå§‹åŒ–ï¼Œæ‰€æœ‰å¿…è¦æ–‡ä»¶æ˜¯å¦å°±ç»ª
        
        Returns:
            bool: æ£€æŸ¥æ˜¯å¦é€šè¿‡
        """
        logger.info("ğŸ” Checking FileManager status...")
        
        errors = []
        
        # æ£€æŸ¥æ•°æ®ç›®å½•
        if not os.path.exists(self.data_dir):
            errors.append(f"Data directory not found: {self.data_dir}")
            logger.error(f"âŒ Data directory: {self.data_dir}")
        else:
            logger.info(f"âœ… Data directory: {self.data_dir}")
        
        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åˆå§‹åŒ–
        files_to_check = [
            (self._detail_log_filename, "Detail log file"),
            (self._keys_valid_filename, "Valid keys file"),
            (self._rate_limited_filename, "Rate limited file"),
            (self._rate_limited_detail_filename, "Rate limited detail file")
        ]
        
        for filename, description in files_to_check:
            if not filename:
                errors.append(f"{description} not initialized")
                logger.error(f"âŒ {description}: Not initialized")
            elif not os.path.exists(filename):
                errors.append(f"{description} not found: {filename}")
                logger.error(f"âŒ {description}: {filename}")
            else:
                logger.info(f"âœ… {description}: {filename}")
        
        # æ£€æŸ¥scanned_shasæ–‡ä»¶
        if os.path.exists(self.scanned_shas_file):
            logger.info(f"âœ… Scanned SHAs file: {self.scanned_shas_file}")
        else:
            logger.info(f"ğŸ“ Scanned SHAs file will be created: {self.scanned_shas_file}")
        
        # æ£€æŸ¥checkpointæ–‡ä»¶
        if os.path.exists(self.checkpoint_file):
            logger.info(f"âœ… Checkpoint file: {self.checkpoint_file}")
        else:
            logger.info(f"ğŸ“ Checkpoint file will be created: {self.checkpoint_file}")
        
        # æ£€æŸ¥æœç´¢æŸ¥è¯¢
        if not hasattr(self, '_search_queries') or not self._search_queries:
            errors.append("Search queries not loaded or empty")
            logger.error("âŒ Search queries: Not loaded or empty")
        else:
            logger.info(f"âœ… Search queries: {len(self._search_queries)} loaded")
        
        if errors:
            logger.error("âŒ FileManager check failed:")
            for error in errors:
                logger.error(f"   - {error}")
            return False
        
        logger.info("âœ… FileManager status check passed")
        return True
    
    # ================================
    # åŠ è½½æ–¹æ³•
    # ================================
    
    def load_checkpoint(self) -> Checkpoint:
        """åŠ è½½checkpointæ•°æ®"""
        checkpoint = Checkpoint()
        
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    checkpoint = Checkpoint.from_dict(data)
            except Exception as e:
                logger.warning(f"Cannot read {self.checkpoint_file}: {e}. Will create new checkpoint.")
        else:
            logger.warning(f"{self.checkpoint_file} not found. Will create new checkpoint.")
            self.save_checkpoint(checkpoint)

        # ä»å•ç‹¬æ–‡ä»¶åŠ è½½scanned_shas
        checkpoint.scanned_shas = self.load_scanned_shas()
        
        return checkpoint
    
    def load_scanned_shas(self) -> Set[str]:
        """ä»æ–‡ä»¶ä¸­åŠ è½½å·²æ‰«æçš„SHAåˆ—è¡¨"""
        scanned_shas = set()
        
        if os.path.isfile(self.scanned_shas_file):
            try:
                with open(self.scanned_shas_file, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            scanned_shas.add(line)
                logger.info(f"Loaded {len(scanned_shas)} scanned SHAs from {self.scanned_shas_file}")
            except Exception as e:
                logger.error(f"Failed to read {self.scanned_shas_file}: {e}")
                logger.info("Using empty scanned SHAs set")
        else:
            logger.info(f"Scanned SHAs file not found: {self.scanned_shas_file}")
            logger.info("Starting with empty scanned SHAs set")
        
        return scanned_shas
    
    def load_search_queries(self, queries_file_path: str) -> List[str]:
        """ä»æ–‡ä»¶ä¸­åŠ è½½æœç´¢æŸ¥è¯¢åˆ—è¡¨"""
        queries = []
        full_path = os.path.join(self.data_dir, queries_file_path)
        
        if not os.path.exists(full_path):
            self._create_default_queries_file(full_path)
        
        try:
            with open(full_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        queries.append(line)
            logger.info(f"Loaded {len(queries)} search queries from {full_path}")
        except Exception as e:
            logger.error(f"Failed to read {full_path}: {e}")
            logger.info("Using empty query list")
        
        return queries
    
    # ================================
    # ä¿å­˜æ–¹æ³•
    # ================================
    
    def save_checkpoint(self, checkpoint: Checkpoint) -> None:
        """ä¿å­˜checkpointæ•°æ®"""
        # 1. ä¿å­˜scanned_shasåˆ°å•ç‹¬æ–‡ä»¶
        self.save_scanned_shas(checkpoint.scanned_shas)
        
        # 2. ä¿å­˜å…¶ä»–æ•°æ®åˆ°checkpoint.json
        try:
            with open(self.checkpoint_file, "w", encoding="utf-8") as f:
                json.dump(checkpoint.to_dict(), f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"Failed to save {self.checkpoint_file}: {e}")
    
    def save_scanned_shas(self, scanned_shas: Set[str]) -> None:
        """ä¿å­˜å·²æ‰«æçš„SHAåˆ—è¡¨åˆ°æ–‡ä»¶"""
        try:
            with open(self.scanned_shas_file, "w", encoding="utf-8") as f:
                f.write("# å·²æ‰«æçš„æ–‡ä»¶SHAåˆ—è¡¨\n")
                f.write("# æ¯è¡Œä¸€ä¸ªSHAï¼Œç”¨äºé¿å…é‡å¤æ‰«æ\n")
                f.write(f"# æœ€åæ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("\n")
                for sha in sorted(scanned_shas):
                    f.write(f"{sha}\n")
            logger.info(f"Saved {len(scanned_shas)} scanned SHAs to {self.scanned_shas_file}")
        except Exception as e:
            logger.error(f"Failed to save scanned SHAs to {self.scanned_shas_file}: {e}")
    
    def save_valid_keys(self, repo_name: str, file_path: str, file_url: str, valid_keys: List[str]) -> None:
        """ä¿å­˜æœ‰æ•ˆçš„APIå¯†é’¥"""
        if not valid_keys or not self._detail_log_filename:
            return
        
        # ä¿å­˜åˆ°è¯¦ç»†æ—¥å¿—æ–‡ä»¶
        with open(self._detail_log_filename, "a", encoding="utf-8") as f:
            f.write(f"TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"URL: {file_url}\n")
            for key in valid_keys:
                f.write(f"KEY: {key}\n")
            f.write("-" * 80 + "\n")
        
        # ä¿å­˜åˆ°keys_validæ–‡ä»¶
        if self._keys_valid_filename:
            with open(self._keys_valid_filename, "a", encoding="utf-8") as f:
                for key in valid_keys:
                    f.write(f"{key}\n")

    def save_rate_limited_keys(self, repo_name: str, file_path: str, file_url: str, rate_limited_keys: List[str]) -> None:
        """ä¿å­˜è¢«é™æµçš„APIå¯†é’¥"""
        if not rate_limited_keys:
            return
        
        # ä¿å­˜è¯¦ç»†ä¿¡æ¯åˆ°è¯¦ç»†æ—¥å¿—æ–‡ä»¶ï¼ˆæ–°æ ¼å¼ï¼‰
        if self._rate_limited_detail_filename:
            with open(self._rate_limited_detail_filename, "a", encoding="utf-8") as f:
                f.write(f"TIME: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"URL: {file_url}\n")
                for key in rate_limited_keys:
                    f.write(f"{key}\n")
                f.write("-" * 80 + "\n")
        
        # ä¿å­˜çº¯å¯†é’¥åˆ°åŸæœ‰æ–‡ä»¶ï¼ˆåªä¿å­˜keyï¼‰
        if self._rate_limited_filename:
            with open(self._rate_limited_filename, "a", encoding="utf-8") as f:
                for key in rate_limited_keys:
                    f.write(f"{key}\n")
    
    def append_scanned_sha(self, sha: str) -> None:
        """è¿½åŠ å•ä¸ªSHAåˆ°æ–‡ä»¶ä¸­"""
        if not sha:
            return
            
        try:
            with open(self.scanned_shas_file, "a", encoding="utf-8") as f:
                f.write(f"{sha}\n")
        except Exception as e:
            logger.error(f"Failed to append SHA {sha} to {self.scanned_shas_file}: {e}")
    
    # ================================
    # æ›´æ–°æ–¹æ³•
    # ================================
    
    def update_dynamic_filenames(self) -> None:
        """æ›´æ–°æ—¶é—´ç›¸å…³çš„æ–‡ä»¶åï¼ˆä¾‹å¦‚æ¯å°æ—¶æ›´æ–°ï¼‰"""
        current_time = datetime.now()
        current_date_str = current_time.strftime('%Y%m%d')
        current_hour_str = current_time.strftime('%H')
        
        # æ›´æ–°keys_validæ–‡ä»¶å
        if self._keys_valid_filename:
            basename = os.path.basename(self._keys_valid_filename)
            if self._need_filename_update(basename, Config.VALID_KEY_PREFIX, current_date_str, current_hour_str):
                old_filename = self._keys_valid_filename
                self._keys_valid_filename = os.path.join(
                    self.data_dir, 
                    f"{Config.VALID_KEY_PREFIX}{current_time.strftime('%Y%m%d_%H')}.txt"
                )
                logger.info(f"Updated keys-valid filename: {old_filename} -> {self._keys_valid_filename}")
        
        # æ›´æ–°rate_limitedæ–‡ä»¶å
        if self._rate_limited_filename:
            basename = os.path.basename(self._rate_limited_filename)
            if self._need_filename_update(basename, Config.RATE_LIMITED_KEY_PREFIX, current_date_str, current_hour_str):
                old_filename = self._rate_limited_filename
                self._rate_limited_filename = os.path.join(
                    self.data_dir, 
                    f"{Config.RATE_LIMITED_KEY_PREFIX}{current_time.strftime('%Y%m%d_%H')}.txt"
                )
                logger.info(f"Updated rate-limited filename: {old_filename} -> {self._rate_limited_filename}")
        
        # æ›´æ–°rate_limited_detailæ–‡ä»¶åï¼ˆæŒ‰æ—¥æœŸåˆ†å‰²ï¼‰
        if self._rate_limited_detail_filename:
            basename = os.path.basename(self._rate_limited_detail_filename)
            if self._need_daily_filename_update(basename, Config.RATE_LIMITED_KEY_DETAIL_PREFIX, current_date_str):
                old_filename = self._rate_limited_detail_filename
                self._rate_limited_detail_filename = os.path.join(
                    self.data_dir, 
                    f"{Config.RATE_LIMITED_KEY_DETAIL_PREFIX}{current_date_str}.log"
                )
                logger.info(f"Updated rate-limited detail filename: {old_filename} -> {self._rate_limited_detail_filename}")
    
    # ================================
    # å±æ€§è®¿é—®å™¨
    # ================================
    
    @property
    def detail_log_filename(self) -> Optional[str]:
        return self._detail_log_filename
    
    @property
    def keys_valid_filename(self) -> Optional[str]:
        return self._keys_valid_filename
    
    @property
    def rate_limited_filename(self) -> Optional[str]:
        return self._rate_limited_filename
    
    @property
    def rate_limited_detail_filename(self) -> Optional[str]:
        return self._rate_limited_detail_filename
    
    # å‘åå…¼å®¹çš„å±æ€§å
    @property
    def main_log_filename(self) -> Optional[str]:
        return self._detail_log_filename
    
    @property
    def keys_only_filename(self) -> Optional[str]:
        return self._keys_valid_filename
    
    def get_search_queries(self) -> List[str]:
        """è·å–æœç´¢æŸ¥è¯¢åˆ—è¡¨"""
        return getattr(self, '_search_queries', [])
    
    # ================================
    # ç§æœ‰è¾…åŠ©æ–¹æ³•
    # ================================
    
    def _create_default_queries_file(self, queries_file: str) -> None:
        """åˆ›å»ºé»˜è®¤çš„æŸ¥è¯¢æ–‡ä»¶"""
        try:
            os.makedirs(os.path.dirname(queries_file), exist_ok=True)
            with open(queries_file, "w", encoding="utf-8") as f:
                f.write("# GitHubæœç´¢æŸ¥è¯¢é…ç½®æ–‡ä»¶\n")
                f.write("# æ¯è¡Œä¸€ä¸ªæŸ¥è¯¢è¯­å¥ï¼Œæ”¯æŒGitHubæœç´¢è¯­æ³•\n")
                f.write("# ä»¥#å¼€å¤´çš„è¡Œä¸ºæ³¨é‡Šï¼Œç©ºè¡Œä¼šè¢«å¿½ç•¥\n")
                f.write("\n")
                f.write("# åŸºç¡€APIå¯†é’¥æœç´¢\n")
                f.write("AIzaSy in:file\n")
            logger.info(f"Created default queries file: {queries_file}")
        except Exception as e:
            logger.error(f"Failed to create default queries file {queries_file}: {e}")
    
    def _need_filename_update(self, basename: str, prefix: str, current_date: str, current_hour: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æ–‡ä»¶å"""
        try:
            time_part = basename[len(prefix):].replace('.txt', '')
            if '_' in time_part:
                filename_date, filename_hour = time_part.split('_', 1)
                return filename_date != current_date or filename_hour != current_hour
        except (IndexError, ValueError):
            pass
        return True
    
    def _need_daily_filename_update(self, basename: str, prefix: str, current_date: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°æŒ‰æ—¥æœŸåˆ†å‰²çš„æ–‡ä»¶å"""
        try:
            time_part = basename[len(prefix):].replace('.log', '')
            return time_part != current_date
        except (IndexError, ValueError):
            pass
        return True
